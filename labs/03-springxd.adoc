== Spring XD overview lab

This lab will go through a general overview using Spring XD. +
For lab and demo purposes, we'll be using a single node system, although Spring XD will scale horizontally as needed.

Requirements

- http://projects.spring.io/spring-xd/[Spring XD] v1.1.0 + installed 

=== Starting Spring XD

Start a single node instance of Spring XD, typing on a terminal:

[source,bash]
----
$ export JAVA_OPTS="-XX:PermSize=512m"
$ xd-singlenode

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.RELEASE                    eXtreme Data


Started : SingleNodeApplication

(...)

1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='9b736207-17df-4ba8-bfb7-8f68a14ab466', attributes={ip=192.168.1.2, host=Fredericos-Air, groups=, pid=9011, id=9b736207-17df-4ba8-bfb7-8f68a14ab466}}
1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
----
Wait for the server startup to be complete, it will take a few seconds. +
Once the server is up and running, let's connect to it from the XD Shell:

[source,bash]
----
$ xd-shell

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.1.0.RELEASE | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type "help".
xd:>
----

As the server is running on the same machine, the XD Shell automatically recognized it and connected. +
We're ready to create our first streams.

First, let's check no stream is currently running:

[source,bash]
----
xd:> stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  ------
----
The list should be empty, meaning no streams currently exist.

=== Creating your first stream

Let's start creating a few very simple data streams that will simply read data from different sources and directly output to select destinations. 

* Reading the system's time and outputing to the log:

On the XD shell window, type:

[source,bash]
----
xd:> stream create stream1 --definition "time | log" --deploy
Created and deployed new stream 'stream1'
----
Check the logs on the window where you started __xd-singlenode__. It should be outputing time ticks. +
Now the _stream list_ command should also identify the stream we created and we should be able to destroy it when desired:

[source,bash]
----
xd:>stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  --------
  stream1      time|log           deployed

xd:>stream destroy stream1
Destroyed stream 'stream1'

----
 
=== Sinking to the filesystem

 Now let's change it a little in order to output to a file instead.
 
* Reading the system's time and outputing to a file:
 
[source,bash]
----
xd:>stream create streamTimeFile --definition "time | file --name=mystream --dir=/tmp" --deploy
Created and deployed new stream 'streamTimeFile'
----

Check the log file on a terminal window and see the time is now being output there: +
 +
[source,bash]
----
$ tail -f /tmp/mystream.out
2015-03-13 23:38:19
2015-03-13 23:38:20

----

=== Listening HTTP port and outputing to a file 

As the next step on this exercise, let's change the _source_ to be something more useful than "time"  
If we specify _http_, XD will automatically start listening to HTTP connections on a port, where we can submit our data to:
 +
[source,bash]
----
xd:>stream create httptofile --definition "http --port=9020 | file --name=fromhttp --dir=/tmp" --deploy
Created and deployed new stream 'httptofile'
----

Now we'll use XD-shell itself to send a JSON object to that HTTP listener:
 +
[source,bash]
----
xd:> http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'

> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

As expected, that data should be now in the /tmp/fromhttp.out file, as specified:
 +
[source,bash]
----
$ cat /tmp/fromhttp.out 
{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
----
 
=== Extracting and transforming JSON data 
 
As a next step, we'll see how XD can be used to easily apply built-in transformations, like extracting specific fields from JSON requests on a data stream. +
Deploy the following stream:
 +
[source,bash]
----
xd:>stream create transform --definition "http --port=9030 | splitter --expression=#jsonPath(payload,'$.Values') | file --name=transform --dir=/tmp" --deploy
Created and deployed new stream 'transform'
----

Let's send the exact same data to this new stream:
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9030' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9030 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

The result, as output on the file specified, is the each value extracted as expected. 
 +
[source,bash]
----
$ cat /tmp/transform.out 
{X=0, Y=1, Z=0, key=0}
{X=1, Y=0, Z=0, key=1}
----
Each value of our JSON object array was extracted as a separate line by the _splitter_ module.
 
Next, we'll add an additional filter to the same definition, extracting only the lines where _Y_ has the value _0_
 +
[source,bash]
----
xd:>stream create transform2 --definition "http --port=9040 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | file --name=transform2 --dir=/tmp" --deploy
Created and deployed new stream 'transform2'
----
 
Sending the exact same data as input, we should only see as output the line with the value specified on the filtering module:
 +
[source,bash]
----
 xd:>http post --target 'http://localhost:9040' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9040 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Checking the output..
[source,bash]
----
$ cat /tmp/transform2.out 
{X=1, Y=0, Z=0, key=1}
----

 Cleaning up everything for the next exercise:
[source,bash]
----
xd:>stream all destroy 
Really destroy all streams? [y, n]: y
Destroyed all streams
----
 
=== Applying an additional transformation and sinking into HDFS 

First we need to make sure Pivotal HD is started using the script provided. 
If you're not sure, just check using _icm_client_
 +
[source,bash]
----
$ icm_client list
Fetching installed clusters
Installed Clusters:
Cluster ID: 1	Cluster Name: pivhd	PHD Version: PHD-2.1.0.0	Status: started
----
In case it's stopped, use the Pivotal HD startup script as linked on the Desktop. 

* Configuring Spring XD for HDFS access

To configure the HDFS Namenode connection within Spring XD just use the command +hadoop config+
 +
[source,bash]
----
xd:>hadoop config fs --namenode hdfs://localhost:8020
----
Check connectivity by listening existing files on HDFS:
 +
[source,bash]
----
xd:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
Found 8 items
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:54 /apps
drwxr-xr-x   - gpadmin hadoop          0 2014-08-24 12:02 /hawq_data
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /hive
drwxr-xr-x   - mapred  hadoop          0 2014-08-24 11:55 /mapred
drwxrwxrwx   - hdfs    hadoop          0 2014-08-24 11:55 /tmp
drwxrwxrwx   - hdfs    hadoop          0 2015-03-14 04:07 /user
drwxrwxrwx   - hdfs    hadoop          0 2015-03-17 08:08 /xd
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /yarn
----
Note the created *xd* directory, where Spring XD will output the streams created.

* Listening HTTP port and outputing to HDFS

Let's modify the previous stream to output to HDFS instead. We could just output the stream as-is to HDFS, but we'll convert it to CSV in order to be able to read with HAWQ.

Creating a simple transformer in groovy:

Create the file _transform.groovy_ in your lab directory with the contents below:
 +
[source,groovy]
----
csv = payload.get('X')+','+payload.get('Y')+','+payload.get('Z')+","+payload.get('key')
return csv
----

That's it. We'll use that as a last step in our transformation, before sinking to HDFS.

From the XD shell window, type (make sure you have the right path for the _transform.groovy_ file):
 +
[source,bash]
----
xd:>stream create transformToHDFS --definition "http --port=9020 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | transform --script='file:/home/gpadmin/BigDataRoadshow/labs/json-to-csv.groovy'| hdfs --directory=/xd --fileName=output " --deploy
Created and deployed new stream 'transformToHDFS'
----
Now send some data in...
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Now let's check the HDFS to see if we have the right data output:
 +
[source,bash]
----
xd:>hadoop fs ls /xd
Found 3 items
drwxrwxrwx   - root    hadoop          0 2015-03-04 14:49 /xd/connected-car
-rw-r--r--   3 gpadmin hadoop          0 2015-03-22 03:47 /xd/output-0.txt.tmp
drwxrwxrwx   - gpadmin hadoop          0 2015-03-17 08:10 /xd/s1
----

While the file is being written to it will have the tmp suffix. When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the tmp suffix. 

When you destroy a stream
 +
[source,bash]
----
xd:>stream destroy --name transformToHDFS
----

and list the stream directory again, in use file suffix doesnâ€™t exist anymore.
Alternatively, as we already mentioned, one can change the rollover size to a smaller value, although for performance eficiency in HDFS bigger files are preferred. +

 
[source,bash]
----
xd:>hadoop fs ls /xd
Found 3 items
drwxrwxrwx   - root    hadoop          0 2015-03-04 14:49 /xd/connected-car
-rw-r--r--   3 gpadmin hadoop         28 2015-03-22 04:00 /xd/output-0.txt
drwxrwxrwx   - gpadmin hadoop          0 2015-03-17 08:10 /xd/s1
----
Now you can list the contents of the file. +
 
[source,bash]
----
xd:>hadoop fs cat /xd/output-0.txt
1,0,0,1
----

=== Tapping and inserting into GemFire

Now we're already splitting, filtering, transforming the input and sinking into HDFS we'll create a parallel stream data delivers the data to GemFire so we can consume that in real-time.

+
[source,bash]
----
gfsh>start locator --name=locator1 --J=-Dgemfire.http-service-port=7575
Starting a GemFire Locator in /home/gpadmin/BigDataRoadshow/locator1...
-
Locator in /home/gpadmin/BigDataRoadshow/locator1 on ip-172-31-18-42.ec2.internal[10334] as locator1 is currently online.
Process ID: 46044
Uptime: 16 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/gpadmin/BigDataRoadshow/locator1/locator1.log
JVM Arguments: -Dgemfire.enable-cluster-configuration=true -Dgemfire.load-cluster-configuration-from-dir=false -Dgemfire.http-service-port=7575 -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/locator-dependencies.jar

Successfully connected to: [host=ip-172-31-18-42.ec2.internal, port=1099]

Cluster configuration service is up and running.
----

+
[source,bash]
----
gfsh>start server --name=server1 --J=-Dgemfire.start-dev-rest-api=true --J=-Dgemfire.http-service-port=8080
Starting a GemFire Server in /home/gpadmin/BigDataRoadshow/server1...
-
Server in /home/gpadmin/BigDataRoadshow/server1 on ip-172-31-18-42.ec2.internal[40404] as server1 is currently online.
Process ID: 46391
Uptime: 7 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/gpadmin/BigDataRoadshow/server1/server1.log
JVM Arguments: -Dgemfire.default.locators=172.31.18.42[10334] -Dgemfire.use-cluster-configuration=true -Dgemfire.start-dev-rest-api=true -Dgemfire.http-service-port=8080 -XX:OnOutOfMemoryError=kill -KILL %p -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/server-dependencies.jar
----
jjj
+
[source,bash]
----
gfsh>create region --name=ValuesFromXD --type=REPLICATE
Member  | Status
-
server1 | Region "/ValuesFromXD" created on "server1"
----

We'll create our tap from the _"filter"_ step on the _"transformToHDFS"_ stream, so GemFire will receive the data already filtered but still in JSon format.
On the XD shell, create the tap below:
+
[source,bash]
----
xd:>stream create gemfireTap --definition "tap:stream:transformToHDFS.filter > object-to-json | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=ValuesFromXD --keyExpression=payload.getField('key')" --deploy
Created and deployed new stream 'gemfireTap'
----

Let's send some data to our original stream again.
+
[source,bash]
----
xd:>http post --target 'http://localhost:9020' --contentType application/json --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (application/json;charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

Now the data should not only be sinked as CSV into HDFS, but also be available in JSon format at GemFire.
Let's confirm GemFire has received the data we've submitted:

----
gfsh>describe region --name=/ValuesFromXD
Name            : ValuesFromXD
Data Policy     : replicate
Hosting Members : server1

Non-Default Attributes Shared By Hosting Members  

 Type  | Name | Value
------ | ---- | -----
Region | size | 1
----

As we can see, there's one value in, which we can easily verify using the GemFire's REST API:
[http://localhost:8888/gemfire-api/v1/ValuesFromXD] 

----
{
  "ValuesFromXD" : [ {
    "X" : 1,
    "Y" : 0,
    "Z" : 0,
    "key" : 1
  } ]
}
----



=== Quering from HAWQ

- TDB -

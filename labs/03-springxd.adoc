== Spring XD overview lab

This lab will go through a general overview using Spring XD. +
For lab and demo purposes, we'll be using a single node system, although Spring XD will scale horizontally as needed.

Requirements

- http://projects.spring.io/spring-xd/[Spring XD] v1.1.0 + installed 

=== Starting Spring XD

Start a single node instance of Spring XD, typing on a terminal:

[source,bash]
----
$ xd-singlenode

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.RELEASE                    eXtreme Data


Started : SingleNodeApplication

(...)

1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='9b736207-17df-4ba8-bfb7-8f68a14ab466', attributes={ip=192.168.1.2, host=Fredericos-Air, groups=, pid=9011, id=9b736207-17df-4ba8-bfb7-8f68a14ab466}}
1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
----
Wait for the server startup to be complete, it will take a few seconds. +
Once the server is up and running, let's connect to it from the XD Shell:

[source,bash]
----
$ xd-shell

 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.1.0.RELEASE | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type "help".
xd:>
----

As the server is running on the same machine, the XD Shell automatically recognized it and connected. +
We're ready to create our first streams.

First, let's check no stream is currently running:

[source,bash]
----
xd:> stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  ------
----
The list should be empty, meaning no streams currently exist.

=== Creating your first stream

Let's start creating a few very simple data streams that will simply read data from different sources and directly output to select destinations. 

* Reading the system's time and outputing to the log:

On the XD shell window, type:

[source,bash]
----
xd:> stream create stream1 --definition "time | log" --deploy
Created and deployed new stream 'stream1'
----
Check the logs on the window where you started __xd-singlenode__. It should be outputing time ticks. +
Now the _stream list_ command should also identify the stream we created and we should be able to destroy it when desired:

[source,bash]
----
xd:>stream list
  Stream Name  Stream Definition  Status
  -----------  -----------------  --------
  stream1      time|log           deployed

xd:>stream destroy stream1
Destroyed stream 'stream1'

----
 
=== Sinking to the filesystem

 Now let's change it a little in order to output to a file instead.
 
* Reading the system's time and outputing to a file:
 
[source,bash]
----
xd:>stream create streamTimeFile --definition "time | file --name=mystream --dir=/tmp" --deploy
Created and deployed new stream 'streamTimeFile'
----

Check the log file on a terminal window and see the time is now being output there: +
 +
[source,bash]
----
$ tail -f /tmp/mystream.out
2015-03-13 23:38:19
2015-03-13 23:38:20

----

=== Listening HTTP port and outputing to a file 

As the next step on this exercise, let's change the _source_ to be something more useful than "time"  
If we specify _http_, XD will automatically start listening to HTTP connections on a port, where we can submit our data to:
 +
[source,bash]
----
xd:>stream create httptofile --definition "http --port=9020 | file --name=fromhttp --dir=/tmp" --deploy
Created and deployed new stream 'httptofile'
----

Now we'll use XD-shell itself to send a JSON object to that HTTP listener:
 +
[source,bash]
----
xd:> http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'

> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

As expected, that data should be now in the /tmp/fromhttp.out file, as specified:
 +
[source,bash]
----
$ cat /tmp/fromhttp.out 
{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
----
 
=== Extracting and transforming JSON data 
 
As a next step, we'll see how XD can be used to easily apply built-in transformations, like extracting specific fields from JSON requests on a data stream. +
Deploy the following stream:
 +
[source,bash]
----
xd:>stream create transform --definition "http --port=9030 | splitter --expression=#jsonPath(payload,'$.Values') | file --name=transform --dir=/tmp" --deploy
Created and deployed new stream 'transform'
----

Let's send the exact same data to this new stream:
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9030' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9030 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----

The result, as output on the file specified, is the each value extracted as expected. 
 +
[source,bash]
----
$ cat /tmp/transform.out 
{X=0, Y=1, Z=0, key=0}
{X=1, Y=0, Z=0, key=1}
----
Each value of our JSON object array was extracted as a separate line by the _splitter_ module.
 
Next, we'll add an additional filter to the same definition, extracting only the lines where _Y_ has the value _0_
 +
[source,bash]
----
xd:>stream create transform2 --definition "http --port=9040 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | file --name=transform2 --dir=/tmp" --deploy
Created and deployed new stream 'transform2'
----
 
Sending the exact same data as input, we should only see as output the line with the value specified on the filtering module:
 +
[source,bash]
----
 xd:>http post --target 'http://localhost:9040' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9040 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Checking the output..
[source,bash]
----
$ cat /tmp/transform2.out 
{X=1, Y=0, Z=0, key=1}
----

 Cleaning up everything for the next exercise:
[source,bash]
----
xd:>stream all destroy 
Really destroy all streams? [y, n]: y
Destroyed all streams
----
 
=== Sinking to HDFS 

First we need to make sure Pivotal HD is started using the script provided. 
If you're not sure, just check using _icm_client_
 +
[source,bash]
----
$ icm_client list
Fetching installed clusters
Installed Clusters:
Cluster ID: 1	Cluster Name: pivhd	PHD Version: PHD-2.1.0.0	Status: started
----
In case it's stopped, use the Pivotal HD startup script as linked on the Desktop. 

* Configuring Spring XD for HDFS access

To configure the HDFS Namenode connection within Spring XD just use the command +hadoop config+
 +
[source,bash]
----
xd:>hadoop config fs --namenode hdfs://localhost:8020
----
Check connectivity by listening existing files on HDFS:
 +
[source,bash]
----
xd:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
Found 8 items
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:54 /apps
drwxr-xr-x   - gpadmin hadoop          0 2014-08-24 12:02 /hawq_data
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /hive
drwxr-xr-x   - mapred  hadoop          0 2014-08-24 11:55 /mapred
drwxrwxrwx   - hdfs    hadoop          0 2014-08-24 11:55 /tmp
drwxrwxrwx   - hdfs    hadoop          0 2015-03-14 04:07 /user
drwxrwxrwx   - hdfs    hadoop          0 2015-03-17 08:08 /xd
drwxr-xr-x   - hdfs    hadoop          0 2014-08-24 11:56 /yarn
----
Note the created *xd* directory, where Spring XD will output the streams created.

* Listening HTTP port and outputing to HDFS

Let's modify the previous stream to output to HDFS instead. 
It only requires a conversion back to JSON and output to HDFS instead of file +
From the XD shell window, type:
 +
[source,bash]
----
xd:>stream create transformToHDFS --definition "http --port=9020 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | object-to-json | hdfs --directory=/xd --fileName=output" --deploy
Created and deployed new stream 'transformToHDFS'
----
Sending some data in...
 +
[source,bash]
----
xd:>http post --target 'http://localhost:9020' --data '{Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}'
> POST (text/plain;Charset=UTF-8) http://localhost:9020 {Values : [{"X":0,"Y":1,"Z":0,"key":0},{"X":1,"Y":0,"Z":0,"key":1}]}
> 200 OK
----
Now let's check the HDFS to see if we have the right data output:
 +
[source,bash]
----
xd:>hadoop fs ls /xd
Found 3 items
drwxrwxrwx   - root    hadoop          0 2015-03-04 14:49 /xd/connected-car
-rw-r--r--   3 gpadmin hadoop          0 2015-03-22 03:47 /xd/output-0.txt.tmp
drwxrwxrwx   - gpadmin hadoop          0 2015-03-17 08:10 /xd/s1
----

While the file is being written to it will have the tmp suffix. When the data written exceeds the rollover size (default 1GB) it will be renamed to remove the tmp suffix. 

When you destroy a stream
 +
[source,bash]
----
xd:>stream destroy --name transformToHDFS
----

and list the stream directory again, in use file suffix doesnâ€™t exist anymore.
Alternatively, as we already mentioned, one can change the rollover size to a smaller value, although for performance eficiency in HDFS bigger files are preferred. 

 +
[source,bash]
----
xd:>hadoop fs ls /xd
Found 3 items
drwxrwxrwx   - root    hadoop          0 2015-03-04 14:49 /xd/connected-car
-rw-r--r--   3 gpadmin hadoop         28 2015-03-22 04:00 /xd/output-0.txt
drwxrwxrwx   - gpadmin hadoop          0 2015-03-17 08:10 /xd/s1
----
Now you can list the contents of the file. +
 
[source,bash]
----
xd:>hadoop fs cat /xd/output-0.txt
{"X":1,"Y":0,"Z":0,"key":1}
----

=== Tapping for GemFire

+
+
gfsh>start locator --name=locator1 --J=-Dgemfire.http-service-port=7575
Starting a GemFire Locator in /home/gpadmin/BigDataRoadshow/locator1...
................................
Locator in /home/gpadmin/BigDataRoadshow/locator1 on ip-172-31-18-42.ec2.internal[10334] as locator1 is currently online.
Process ID: 46044
Uptime: 16 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/gpadmin/BigDataRoadshow/locator1/locator1.log
JVM Arguments: -Dgemfire.enable-cluster-configuration=true -Dgemfire.load-cluster-configuration-from-dir=false -Dgemfire.http-service-port=7575 -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/locator-dependencies.jar

Successfully connected to: [host=ip-172-31-18-42.ec2.internal, port=1099]

Cluster configuration service is up and running.



+
gfsh>start server --name=server1 --J=-Dgemfire.start-dev-rest-api=true --J=-Dgemfire.http-service-port=8080
Starting a GemFire Server in /home/gpadmin/BigDataRoadshow/server1...
..............
Server in /home/gpadmin/BigDataRoadshow/server1 on ip-172-31-18-42.ec2.internal[40404] as server1 is currently online.
Process ID: 46391
Uptime: 7 seconds
GemFire Version: 8.1.0
Java Version: 1.7.0_45
Log File: /home/gpadmin/BigDataRoadshow/server1/server1.log
JVM Arguments: -Dgemfire.default.locators=172.31.18.42[10334] -Dgemfire.use-cluster-configuration=true -Dgemfire.start-dev-rest-api=true -Dgemfire.http-service-port=8080 -XX:OnOutOfMemoryError=kill -KILL %p -Dgemfire.launcher.registerSignalHandlers=true -Djava.awt.headless=true -Dsun.rmi.dgc.server.gcInterval=9223372036854775806
Class-Path: /opt/pivotal/gemfire/Pivotal_GemFire_810/lib/gemfire.jar:/opt/pivotal/gemfire/Pivotal_GemFire_810/lib/server-dependencies.jar
+

gfsh>create region --name=ValuesFromXD --type=REPLICATE
Member  | Status
------- | -------------------------------------------
server1 | Region "/ValuesFromXD" created on "server1"
+

xd:>stream create gemfireTap --definition "tap:stream:transformToHDFS.filter > gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=ValuesFromXD --keyExpression=payload.getField('key')" --deploy
Created and deployed new stream 'gemfireTap'



+
+
+
+


=== Applying a simple Data Filtering / Transformation

For this example, we'll use Yahoo Query Language (YQL) REST interface to query quotes from our favorite stock.

Using our browser, open https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20yahoo.finance.quote%20where%20symbol%20in%20(%22EMC%22)&env=store%3A%2F%2Fdatatables.org%2Falltableswithkeys&format=json

{"query":
     {"count":1,"created":"2015-03-16T07:05:57Z",
      "lang":"en-US",
      "results":{
          "quote":{
              "symbol":"EMC",
              "AverageDailyVolume":
              "15112500",
              "Change":"-0.17",
              "DaysLow":"25.49",
              "DaysHigh":"26.08",
              "YearLow":"24.92",
              "YearHigh":"30.92",
              "MarketCapitalization":"51.69B",
              "LastTradePriceOnly":"26.00",
              "DaysRange":"25.49 - 26.08",
              "Name":"EMC Corporation Common Stock",
              "Symbol":"EMC",
              "Volume":"18974738",
              "StockExchange":"NYQ"
            }
         }
       }
  }

Note the response in JSON format. We can query the same data at each 3 seconds and output the result to XD's log file creating the stream:

[source,shell]
----
xd:> stream create stream1 --definition "trigger --fixedDelay=10 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\"MSFT\")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | log" --deploy 
----

However, we're only interested on the *results* object of the json response, more specifically the *results.quote* object.

So let's apply add quick filtering to only have that part of the result:  +splitter --expression=#jsonPath(payload,'$.query.results.quote')+ 

[source,shell]
----
xd:> stream create stream1 --definition "trigger --fixedDelay=10 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\"MSFT\")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | log" --deploy 
----

You should see in the log only the stock quote data, eliminating all the header we were not interested on.

=== Enriching with a simple Groovy script

Create a file called __transform.groovy__  as following:

[source,groovy]
----
payload.put("timestamp", headers.get('timestamp'))
return payload
----

Those simple two lines will be responsible for adding the timestamp of the message to the JSON object containing the quote, as we'll use that to store in our in-memory grid.
To verify the result, use the stream:

[source,shell]
----
xd:> stream create stream1 --definition "trigger --fixedDelay=10 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\"MSFT\")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') |  transform --script='file:[path_to_your_file]/transform.groovy' | log" --deploy 
----

=== Sinking the data into GemFire

Start a GemFire server:

[source,shell]
----
$ gfsh

gfsh$ 

BLA BLA BLA
----

Create a region called Stocks to hold the information Spring XD will be sinking there:

[source,shell]
----
gfsh $ create region 

BLA BLA BLA
----


Create the Spring XD stream that outputs the data already filtered and enriched into GemFire:

[source,shell]
----
stream create stream1 --definition "trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\"MSFT\")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | transform --script='file:/Users/fmelo/FinanceStream/transform.groovy'| gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('timestamp')" --deploy
----


=== Creating a second stream to read data from GemFire 

[source,shell]
----
stream create stream2 --definition "gemfire --regionName=Stocks --useLocator=true --host=localhost --port=10334 | shell --command='Rscript /Users/fmelo/FinanceStream/test.R' | log " --deploy
----




Troubleshoot any issues until you have your first custom bosh release deployment!! (there are some corrections to be done!) The troubleshooting part is very important!! That's how you learn!!

Hints: 

- The failing canary will be kept by bosh for troubleshooting purposes
- When testing, subsequent deployments should be done using __bosh deploy --recreate__ , otherwise new additional VMs will be created (canary won't be updated unless __--recreate__ is specified).
- Check logs and try to understand what's going on. You can try to run the commands yourself once logged into the VM to understand what's wrong.
- Dr Nick created a project called https://github.com/drnic/bosh-solo[BOSH-Solo] which helps testing BOSH releases. You might want to give it a try! (not mandatory)

Good luck!! Next challenge is adding a Service Broker capable of provisioning PostgreSQL instances to the release you just created :)

If you'd like to check the solution for this lab, clone this repo: https://github.com/Pivotal-Field-Engineering/postgres-bosh-release[postgres-bosh-release]


xd:>stream create gemfireTap --definition "http --port=9020 | splitter --expression=#jsonPath(payload,'$.Values') | filter --expression=#jsonPath(payload,'$.Y').equals(0) | object-to-json | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=ValuesFromXD --keyExpression=payload.getField('key')" --deploy
